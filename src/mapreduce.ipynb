{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Intensive Computing 2024S\n",
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_data = '../data/reviews_devset.json'\n",
    "all_data = ''\n",
    "stopwords = '../data/stopwords.txt'\n",
    "temp_file = '../data/temp.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mrjob in c:\\users\\corpa\\master\\2024s\\data intensive computing\\exercise 1\\solution\\.venv\\lib\\site-packages (0.7.4)\n",
      "Requirement already satisfied: mr3px in c:\\users\\corpa\\master\\2024s\\data intensive computing\\exercise 1\\solution\\.venv\\lib\\site-packages (0.5.1)\n",
      "Requirement already satisfied: PyYAML>=3.10 in c:\\users\\corpa\\master\\2024s\\data intensive computing\\exercise 1\\solution\\.venv\\lib\\site-packages (from mrjob) (6.0.1)\n"
     ]
    }
   ],
   "source": [
    "# %%bash\n",
    "\n",
    "!pip3 install mrjob mr3px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First MrJob \n",
    "-> maps the data into <key, 1> pairs where <key> is the category and it has one review associated to it (initalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting job1.py\n"
     ]
    }
   ],
   "source": [
    "%%file job1.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.util import log_to_stream, log_to_null\n",
    "from mrjob.protocol import JSONValueProtocol\n",
    "from mr3px.csvprotocol import CsvProtocol\n",
    "import csv\n",
    "import logging\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "class MrJob1(MRJob):\n",
    "    \n",
    "    INPUT_PROTOCOL = JSONValueProtocol\n",
    "    OUTPUT_PROTOCOL = CsvProtocol\n",
    "    \n",
    "    def set_up_logging(cls, quiet=False, verbose=False, stream=None):  \n",
    "        log_to_stream(name='mrjob', debug=verbose, stream=stream)\n",
    "        log_to_stream(name='__main__', debug=verbose, stream=stream)\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        category = line.get('category')\n",
    "        if category is not None:\n",
    "            yield (category, 1)\n",
    "        yield ('#reviews', 1)\n",
    "    def combiner(self, key, valuelist):\n",
    "        total = sum(valuelist)\n",
    "        yield (key, total)\n",
    "\n",
    "    def reducer(self, key, valuelist):\n",
    "        total = sum(valuelist)\n",
    "        yield None, (key, total)  \n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper = self.mapper, \n",
    "                combiner = self.combiner,\n",
    "                reducer = self.reducer)\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MrJob1.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running a local MRjob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory C:\\Users\\corpa\\AppData\\Local\\Temp\\job1.corpa.20240422.232731.339885\n",
      "Running step 1 of 1...\n",
      "job output is in C:\\Users\\corpa\\AppData\\Local\\Temp\\job1.corpa.20240422.232731.339885\\output\n",
      "Streaming final output from C:\\Users\\corpa\\AppData\\Local\\Temp\\job1.corpa.20240422.232731.339885\\output...\n",
      "Removing temp directory C:\\Users\\corpa\\AppData\\Local\\Temp\\job1.corpa.20240422.232731.339885...\n"
     ]
    }
   ],
   "source": [
    "!python job1.py $partial_data  > $temp_file\n",
    "\n",
    "#!python job1.py $all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running a Hadoop job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second MrJob \n",
    "-> ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting job2.py\n"
     ]
    }
   ],
   "source": [
    "%%file job2.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.util import log_to_stream, log_to_null\n",
    "from mrjob.protocol import JSONValueProtocol, TextProtocol\n",
    "import logging\n",
    "import json\n",
    "import re\n",
    "from csv import reader\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "pathway = '../data/'\n",
    "\n",
    "\n",
    "# read in output from previous job as dictionary\n",
    "rev_per_cat = {}\n",
    "with open(pathway + 'temp.csv', 'r') as read_obj:\n",
    "    csv_reader = reader(read_obj)\n",
    "    for row in csv_reader:\n",
    "        rev_per_cat[row[0]] = int(row[1])\n",
    "\n",
    "\n",
    "# read stopwords\n",
    "stopwords = set(word.strip() for word in open(pathway + 'stopwords.txt'))\n",
    "\n",
    "class MRJob2(MRJob):\n",
    "    \n",
    "    FILES = [pathway + 'stopwords.txt', pathway + 'temp.csv']\n",
    "    INPUT_PROTOCOL = JSONValueProtocol\n",
    "    OUTPUT_PROTOCOL = TextProtocol\n",
    "    \n",
    "    def set_up_logging(cls, quiet=False, verbose=False, stream=None):  \n",
    "        log_to_stream(name='mrjob', debug=verbose, stream=stream)\n",
    "        log_to_stream(name='__main__', debug=verbose, stream=stream)\n",
    "\n",
    "    def mapper_preprocessing(self, _, line):\n",
    "        ''' \n",
    "        This function preprocesses the data and counts the number of times each term appears in each category\n",
    "        It takes in a line of data and returns a tuple with a key-value pair where the key is a tuple of the term and the category\n",
    "        and the value is 1'''\n",
    "        \n",
    "        review = line['reviewText']\n",
    "        category = line['category']\n",
    "        \n",
    "        # tokenize\n",
    "        tokens = re.split(r'[ \\t\\d()\\[\\]{}.!?,;:+=\\-_\"\\'~#@&*%€$§\\/]+', review)\n",
    "        \n",
    "        # case folding\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "        \n",
    "        # stopword filtering\n",
    "        filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "\n",
    "        for token in filtered_tokens:\n",
    "            yield (token, category), 1\n",
    "            \n",
    "    def combiner_count_term_per_category(self, key, valuelist):\n",
    "        total = sum(valuelist)\n",
    "        yield key, total\n",
    "    \n",
    "    def reducer_count_term_per_category(self, key, valuelist):\n",
    "        '''\n",
    "        This function takes in a key-value pair where the key is a tuple of the term and the category\n",
    "        and the value is a list of 1s. It returns a key-value pair where the key is the term and the value is the total count of the term in the category\n",
    "        '''\n",
    "        total = sum(valuelist)\n",
    "        yield key, total             \n",
    "\n",
    "    def mapper_transform(self, key, category_count):\n",
    "        '''\n",
    "        This mapper creates a new key-value pair where the key is the term and the value is a dictionary with the category as the key and the count as the value\n",
    "        '''\n",
    "        term, category = key\n",
    "        yield term, {category: category_count}\n",
    "\n",
    "    def reducer_count_terms(self, term, list_of_dicts):\n",
    "        '''\n",
    "        This function takes in a key-value pair where the key is the term and the value is a list of dictionaries where each dictionary has the category as the key and the count as the value\n",
    "        It returns a key-value pair where the key is the term and the value is a tuple with the total count of the term and a dictionary with the category as the key and the count as the value\n",
    "        '''\n",
    "        term_dictionary = {k:v for item in list_of_dicts for (k,v) in item.items()}\n",
    "        values = term_dictionary.values()\n",
    "        total = sum(values)\n",
    "        \n",
    "        yield term, (total, term_dictionary)\n",
    "        \n",
    "    def mapper_chi_squared_statistic(self, term, sum_dictionary):\n",
    "        '''\n",
    "        This function takes in a key-value pair where the key is the term and the value is a tuple with the total count of the term and a dictionary with the category as the key and the count as the value\n",
    "        It returns a key-value pair where the key is the category and the value is a dictionary with the term as the key and the chi-squared value as the value\n",
    "        '''\n",
    "        \n",
    "        count_all_cat, count_each_cat = sum_dictionary\n",
    "        \n",
    "        for cat, count_cat in count_each_cat.items():\n",
    "            N = rev_per_cat['#reviews']         \n",
    "            A = count_cat\n",
    "            B = count_all_cat - A\n",
    "            C = rev_per_cat[cat] - A\n",
    "            D = N - rev_per_cat[cat] - B\n",
    "            chi_squared = (N * ((A*D - B*C)**2)) / ((A+B) * (A+C) * (B+D) * (C+D))\n",
    "            \n",
    "            yield cat, {term: chi_squared}\n",
    "    \n",
    "    def reducer_top75_terms(self, category, term_dictionary):\n",
    "        '''\n",
    "        This function takes in a key-value pair where the key is the category and the value is a list of dictionaries where each dictionary has the term as the key and the chi-squared value as the value\n",
    "        It returns a key-value pair where the key is the category and the value is a dictionary with the top 75 terms and their chi-squared values\n",
    "        '''\n",
    "        term_dictionary = {k:v for list_item in term_dictionary for (k,v) in list_item.items()}\n",
    "        top75 = {k:v for k,v in sorted(term_dictionary.items(), key = lambda x: x[1], reverse = True)[:75]}     \n",
    "        top75_keys = sorted(top75.keys())\n",
    "        \n",
    "        yield category, top75\n",
    "        yield \"category\", top75_keys\n",
    "    \n",
    "    def reducer_intermediary_result(self, category, top75__terms):\n",
    "        '''\n",
    "        This function takes in a key-value pair where the key is the category and the value is a list of dictionaries where each dictionary has the term as the key and the chi-squared value as the value\n",
    "        It returns a key-value pair where the key is the category and the value is a dictionary with the top 75 terms and their chi-squared values\n",
    "        '''\n",
    "        if category != \"category\":\n",
    "            top75__terms = {k:v for list_item in top75__terms for (k,v) in list_item.items()}\n",
    "            top75__terms_formatted = str(top75__terms).replace(\" \", \"\").replace(\",\", \" \").replace(\"\\'\", \"\").strip(\"\\{\\}\")\n",
    "            yield \"\", (category, top75__terms_formatted)\n",
    "        else:\n",
    "            one_list = []\n",
    "            for item in top75__terms:\n",
    "                one_list.append(item)\n",
    "            one_list = [l for sublist in one_list for l in sublist]\n",
    "            merged_dict = str(sorted(list(set(one_list)))).replace(\"\\'\", \"\").replace(\",\", \" \").strip(\"\\[\\]\")\n",
    "            yield \"\", (category, merged_dict)\n",
    "\n",
    "    def reducer_final_result(self, _, output_list):\n",
    "        '''\n",
    "        This function takes in a key-value pair where the key is an empty string and the value is a list of tuples where each tuple has the category as the first element and the top 75 terms and their chi-squared values as the second element\n",
    "        It returns a key-value pair where the key is the category and the value is a dictionary with the top 75 terms and their chi-squared values, sorted in alphabetic order by category\n",
    "        '''\n",
    "        for key, val in sorted(output_list):\n",
    "            if key != \"category\":\n",
    "                yield key, val\n",
    "            else:\n",
    "                yield val, \"\"\n",
    "\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                # preprocess and initialize with 1 the number of times each term appears in each category\n",
    "                mapper  = self.mapper_preprocessing, \n",
    "                combiner = self.combiner_count_term_per_category,\n",
    "                reducer = self.reducer_count_term_per_category),\n",
    "            MRStep(\n",
    "                # transform the data and count the total number of times each term appears\n",
    "                mapper  = self.mapper_transform, \n",
    "                reducer = self.reducer_count_terms),\n",
    "            MRStep(\n",
    "                # calculate the chi-squared statistic for each term in each category\n",
    "                mapper  = self.mapper_chi_squared_statistic,\n",
    "                reducer = self.reducer_top75_terms),\n",
    "            MRStep(\n",
    "                # format the output\n",
    "                reducer = self.reducer_intermediary_result),\n",
    "            MRStep(\n",
    "                # format the output\n",
    "                reducer = self.reducer_final_result)\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRJob2.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python job2.py $partial_data > output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
